#!/usr/bin/env python3
"""
Real-Data Run for Solar Seed
============================

Reproducible, citable analysis run on real AIA data.

Execution:
    python -m solar_seed.real_run --hours 6
    python -m solar_seed.real_run --start "2024-01-15T00:00:00" --hours 6

Output:
    results/real_run/
    â”œâ”€â”€ timeseries.csv          # MI timeseries
    â”œâ”€â”€ controls_summary.json   # Control tests
    â”œâ”€â”€ spatial_maps.txt        # Spatial analysis
    â””â”€â”€ run_metadata.json       # Configuration & reproducibility
"""

import argparse
import json
import csv
from datetime import datetime, timedelta
from pathlib import Path
from dataclasses import dataclass, asdict, field
from typing import Optional, List, Tuple
import numpy as np
from numpy.typing import NDArray

from solar_seed.mutual_info import mutual_information
from solar_seed.radial_profile import prepare_pair_for_residual_mi
from solar_seed.control_tests import (
    run_all_controls,
    time_shift_null,
    sector_ring_shuffle_test,
    AllControlsResult
)
from solar_seed.spatial_analysis import (
    compute_spatial_residual_mi_map,
    mi_map_to_ascii,
    find_top_hotspots
)


# ============================================================================
# CONFIGURATION
# ============================================================================

@dataclass
class RunConfig:
    """Configuration for the real run."""
    wavelength_1: int = 193
    wavelength_2: int = 211
    start_time: Optional[str] = None  # ISO format, None = now - hours
    hours: float = 6.0
    cadence_minutes: int = 12  # AIA synoptic cadence
    output_dir: str = "results/real_run"
    bins: int = 64
    n_shuffles: int = 100
    seed: int = 42


@dataclass
class TimePointResult:
    """Result for a timepoint."""
    timestamp: str

    # Original MI
    mi_original: float
    nmi_original: float

    # Residual MI
    mi_residual: float
    nmi_residual: float

    # New metrics
    mi_ratio: float  # mi_residual / mi_original
    delta_mi_ring: float  # mi_residual - mi_ring_shuffled
    delta_mi_sector: float  # mi_residual - mi_sector_shuffled

    # Null model
    mi_null_mean: float
    mi_null_std: float
    z_score: float

    # Shape info
    shape: Tuple[int, int] = (0, 0)


@dataclass
class RunResult:
    """Overall result of the run."""
    config: RunConfig
    timeseries: List[TimePointResult]
    controls: Optional[dict]
    spatial: Optional[dict]

    # Aggregated statistics
    mean_mi_ratio: float = 0.0
    std_mi_ratio: float = 0.0
    mean_delta_mi_ring: float = 0.0
    mean_delta_mi_sector: float = 0.0

    run_timestamp: str = ""
    duration_seconds: float = 0.0


# ============================================================================
# DATA LOADING
# ============================================================================

def load_aia_pair_sunpy(
    wavelength_1: int,
    wavelength_2: int,
    time_str: str
) -> Tuple[Optional[NDArray], Optional[NDArray], dict]:
    """
    Loads an AIA image pair via SunPy/Fido.

    Args:
        wavelength_1: First wavelength (e.g. 193)
        wavelength_2: Second wavelength (e.g. 211)
        time_str: Timestamp (ISO format)

    Returns:
        (image_1, image_2, metadata) or (None, None, {}) on error
    """
    try:
        from sunpy.net import Fido, attrs as a
        import sunpy.map
        import astropy.units as u
        from datetime import datetime, timedelta

        # Parse time and create search window (Â±5 min)
        t = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
        t_start = t - timedelta(minutes=5)
        t_end = t + timedelta(minutes=5)

        images = []
        metadata = {"timestamp": time_str, "wavelengths": [wavelength_1, wavelength_2]}

        for wl in [wavelength_1, wavelength_2]:
            result = Fido.search(
                a.Time(t_start.isoformat(), t_end.isoformat()),
                a.Instrument("aia"),
                a.Wavelength(wl * u.angstrom),
            )

            if len(result) == 0 or len(result[0]) == 0:
                return None, None, {}

            # Load first result
            files = Fido.fetch(result[0, 0], path="data/aia/{file}")
            if not files:
                return None, None, {}

            aia_map = sunpy.map.Map(files[0])
            images.append(aia_map.data.astype(np.float64))

            metadata[f"file_{wl}"] = str(files[0])
            metadata[f"date_{wl}"] = str(aia_map.date)

        return images[0], images[1], metadata

    except ImportError:
        print("  âš ï¸  SunPy not installed")
        return None, None, {}
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return None, None, {}


def generate_synthetic_timeseries(
    n_points: int,
    shape: Tuple[int, int] = (512, 512),
    extra_correlation: float = 0.3,
    seed: int = 42
) -> List[Tuple[NDArray, NDArray, str]]:
    """
    Generates synthetic timeseries for testing.

    Simulates temporal variation through slightly different seeds.
    """
    from solar_seed.data_loader import generate_synthetic_sun

    results = []
    base_time = datetime.now()

    for i in range(n_points):
        timestamp = (base_time + timedelta(minutes=12 * i)).isoformat()

        # Vary extra_correlation slightly over time
        corr = extra_correlation + 0.1 * np.sin(2 * np.pi * i / n_points)

        data_1, data_2 = generate_synthetic_sun(
            shape=shape,
            extra_correlation=max(0, corr),
            n_active_regions=5 + i % 3,
            seed=seed + i
        )

        results.append((data_1, data_2, timestamp))

    return results


# ============================================================================
# ANALYSIS
# ============================================================================

def analyze_timepoint(
    image_1: NDArray[np.float64],
    image_2: NDArray[np.float64],
    timestamp: str,
    config: RunConfig
) -> TimePointResult:
    """
    Analyzes a single timepoint.

    Calculates:
    - Original MI
    - Residual MI (after geometry subtraction)
    - MI Ratio (residual / original)
    - Î”MI_ring (structure beyond radial statistics)
    - Î”MI_sector (structure beyond radial+azimuthal order)
    """
    from solar_seed.null_model import compute_null_distribution, compute_z_score

    # Original MI
    mi_original = mutual_information(image_1, image_2, bins=config.bins)
    nmi_original = mi_original / 6.0  # Rough normalization

    # Residual MI
    res_1, res_2, _ = prepare_pair_for_residual_mi(image_1, image_2)
    mi_residual = mutual_information(res_1, res_2, bins=config.bins)
    nmi_residual = mi_residual / 6.0

    # Ratio
    mi_ratio = mi_residual / mi_original if mi_original > 0 else 0.0

    # Ring and sector shuffles for delta metrics
    sector_result = sector_ring_shuffle_test(
        image_1, image_2,
        n_rings=20,
        n_sectors=16,
        seed=config.seed,
        bins=config.bins
    )

    delta_mi_ring = mi_residual - sector_result.mi_ring_shuffled
    delta_mi_sector = mi_residual - sector_result.mi_sector_shuffled

    # Null model for residual
    mi_null_mean, mi_null_std, _ = compute_null_distribution(
        res_1, res_2,
        n_shuffles=min(50, config.n_shuffles),  # Faster for timeseries
        bins=config.bins,
        seed=config.seed
    )
    z_score = compute_z_score(mi_residual, mi_null_mean, mi_null_std)

    return TimePointResult(
        timestamp=timestamp,
        mi_original=mi_original,
        nmi_original=nmi_original,
        mi_residual=mi_residual,
        nmi_residual=nmi_residual,
        mi_ratio=mi_ratio,
        delta_mi_ring=delta_mi_ring,
        delta_mi_sector=delta_mi_sector,
        mi_null_mean=mi_null_mean,
        mi_null_std=mi_null_std,
        z_score=z_score,
        shape=image_1.shape
    )


def run_spatial_analysis(
    image_1: NDArray[np.float64],
    image_2: NDArray[np.float64],
    grid_size: Tuple[int, int] = (8, 8)
) -> dict:
    """
    Runs spatial analysis.
    """
    result = compute_spatial_residual_mi_map(
        image_1, image_2,
        grid_size=grid_size,
        bins=32
    )

    hotspots = find_top_hotspots(result.residual, n=5)

    return {
        "grid_size": grid_size,
        "original_mi_mean": result.original.mi_mean,
        "original_mi_std": result.original.mi_std,
        "residual_mi_mean": result.residual.mi_mean,
        "residual_mi_std": result.residual.mi_std,
        "mean_reduction_percent": float(np.nanmean(result.mi_reduction_percent)),
        "hotspot": result.residual.hotspot_idx,
        "hotspot_mi": result.residual.hotspot_value,
        "top_5_hotspots": [(list(idx), val) for idx, val in hotspots],
        "original_map_ascii": mi_map_to_ascii(result.original.mi_map),
        "residual_map_ascii": mi_map_to_ascii(result.residual.mi_map)
    }


# ============================================================================
# REPORTING
# ============================================================================

def save_timeseries_csv(
    timeseries: List[TimePointResult],
    filepath: Path
) -> None:
    """Saves timeseries as CSV."""

    with open(filepath, 'w', newline='') as f:
        writer = csv.writer(f)

        # Header
        writer.writerow([
            "timestamp",
            "mi_original",
            "mi_residual",
            "mi_ratio",
            "delta_mi_ring",
            "delta_mi_sector",
            "z_score",
            "mi_null_mean",
            "mi_null_std"
        ])

        # Data
        for tp in timeseries:
            writer.writerow([
                tp.timestamp,
                f"{tp.mi_original:.6f}",
                f"{tp.mi_residual:.6f}",
                f"{tp.mi_ratio:.4f}",
                f"{tp.delta_mi_ring:.6f}",
                f"{tp.delta_mi_sector:.6f}",
                f"{tp.z_score:.2f}",
                f"{tp.mi_null_mean:.6f}",
                f"{tp.mi_null_std:.6f}"
            ])


def save_controls_json(
    controls: AllControlsResult,
    filepath: Path
) -> None:
    """Saves control tests as JSON."""

    # Helper to convert numpy types to Python native
    def to_native(val):
        if hasattr(val, 'item'):
            return val.item()
        return val

    data = {
        "c1_time_shift": {
            "mi_original": to_native(controls.c1_time_shift.mi_original),
            "mi_shifted": to_native(controls.c1_time_shift.mi_shifted),
            "reduction_percent": to_native(controls.c1_time_shift.mi_reduction_percent),
            "passed": to_native(controls.c1_time_shift.passed)
        },
        "c2_ring_shuffle": {
            "mi_original": to_native(controls.c2_ring_shuffle.mi_original),
            "mi_ring_shuffled": to_native(controls.c2_ring_shuffle.mi_ring_shuffled),
            "mi_global_shuffled": to_native(controls.c2_ring_shuffle.mi_global_shuffled),
            "ring_reduction_percent": to_native(controls.c2_ring_shuffle.ring_reduction_percent),
            "global_reduction_percent": to_native(controls.c2_ring_shuffle.global_reduction_percent)
        },
        "c3_blur_match": {
            "mi_original": to_native(controls.c3_blur_match.mi_original),
            "mi_blurred": to_native(controls.c3_blur_match.mi_blurred),
            "change_percent": to_native(controls.c3_blur_match.mi_change_percent),
            "blur_sigma": to_native(controls.c3_blur_match.blur_sigma),
            "stable": to_native(controls.c3_blur_match.stable)
        },
        "c4_co_alignment": {
            "mi_at_zero": to_native(controls.c4_co_alignment.mi_at_zero),
            "mi_at_max": to_native(controls.c4_co_alignment.mi_at_max),
            "max_shift": [to_native(x) for x in controls.c4_co_alignment.max_shift],
            "centered": to_native(controls.c4_co_alignment.centered)
        },
        "all_passed": to_native(controls.all_passed)
    }

    # Add sector shuffle if available
    if hasattr(controls, 'c2_sector_shuffle'):
        data["c2_sector_shuffle"] = {
            "mi_sector_shuffled": controls.c2_sector_shuffle.mi_sector_shuffled,
            "sector_reduction_percent": controls.c2_sector_shuffle.sector_reduction_percent
        }

    with open(filepath, 'w') as f:
        json.dump(data, f, indent=2)


def save_spatial_maps(
    spatial: dict,
    filepath: Path
) -> None:
    """Saves spatial analysis as text."""

    with open(filepath, 'w') as f:
        f.write("SPATIAL MI ANALYSIS\n")
        f.write("=" * 60 + "\n\n")

        f.write(f"Grid: {spatial['grid_size'][0]}x{spatial['grid_size'][1]}\n\n")

        f.write("ORIGINAL MI:\n")
        f.write(f"  Mean: {spatial['original_mi_mean']:.4f} Â± {spatial['original_mi_std']:.4f}\n\n")
        f.write(spatial['original_map_ascii'] + "\n\n")

        f.write("RESIDUAL MI:\n")
        f.write(f"  Mean: {spatial['residual_mi_mean']:.4f} Â± {spatial['residual_mi_std']:.4f}\n")
        f.write(f"  Reduction: {spatial['mean_reduction_percent']:.1f}%\n\n")
        f.write(spatial['residual_map_ascii'] + "\n\n")

        f.write("TOP 5 RESIDUAL HOTSPOTS:\n")
        for i, (idx, val) in enumerate(spatial['top_5_hotspots'], 1):
            f.write(f"  {i}. Cell {idx}: MI={val:.4f}\n")


def save_run_metadata(
    result: RunResult,
    filepath: Path
) -> None:
    """Saves run metadata for reproducibility."""

    metadata = {
        "run_timestamp": result.run_timestamp,
        "duration_seconds": result.duration_seconds,
        "config": asdict(result.config),
        "summary": {
            "n_timepoints": len(result.timeseries),
            "mean_mi_ratio": result.mean_mi_ratio,
            "std_mi_ratio": result.std_mi_ratio,
            "mean_delta_mi_ring": result.mean_delta_mi_ring,
            "mean_delta_mi_sector": result.mean_delta_mi_sector,
            "controls_passed": result.controls.get("all_passed", False) if result.controls else None
        },
        "interpretation": {
            "mi_ratio_meaning": "MI_residual / MI_original - how much MI remains after geometry subtraction",
            "delta_mi_ring_meaning": "MI_residual - MI_ring_shuffle - structure beyond radial statistics",
            "delta_mi_sector_meaning": "MI_residual - MI_sector_shuffle - true local structure"
        }
    }

    with open(filepath, 'w') as f:
        json.dump(metadata, f, indent=2)


def print_run_summary(result: RunResult) -> None:
    """Prints summary of the run."""

    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     ğŸŒ REAL-RUN SUMMARY ğŸŒ±                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Time period:  {result.config.hours} hours
  Wavelengths:  {result.config.wavelength_1} Ã… + {result.config.wavelength_2} Ã…
  Timepoints:   {len(result.timeseries)}
  Duration:     {result.duration_seconds:.1f} seconds

  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  KEY METRICS:

  MI Ratio (residual/original):
    Mean: {result.mean_mi_ratio:.4f} Â± {result.std_mi_ratio:.4f}
    â†’ {result.mean_mi_ratio*100:.1f}% of original MI remains after geometry subtraction

  Î”MI_ring (structure beyond radial statistics):
    Mean: {result.mean_delta_mi_ring:.4f} bits
    â†’ Positive = true azimuthal/local structure present

  Î”MI_sector (true local structure):
    Mean: {result.mean_delta_mi_sector:.4f} bits
    â†’ Positive = structure beyond radial+azimuthal order

  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  OUTPUT FILES:
    {result.config.output_dir}/timeseries.csv
    {result.config.output_dir}/controls_summary.json
    {result.config.output_dir}/spatial_maps.txt
    {result.config.output_dir}/run_metadata.json

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


# ============================================================================
# MAIN RUN
# ============================================================================

def run_real_analysis(
    config: RunConfig,
    use_synthetic: bool = False,
    verbose: bool = True
) -> RunResult:
    """
    Runs the complete real-run analysis.

    Args:
        config: Run configuration
        use_synthetic: Use synthetic data (for testing)
        verbose: Verbose output
    """
    import time
    start_time = time.time()
    run_timestamp = datetime.now().isoformat()

    # Create output directory
    output_dir = Path(config.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    if verbose:
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸŒ SOLAR SEED REAL-RUN ğŸŒ±                           â•‘
â•‘                                                                        â•‘
â•‘  Reproducible analysis on {"real" if not use_synthetic else "synthetic"} AIA data                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)

    # Calculate number of timepoints
    n_points = int(config.hours * 60 / config.cadence_minutes)

    if verbose:
        print(f"  Configuration:")
        print(f"    Wavelengths: {config.wavelength_1} Ã… + {config.wavelength_2} Ã…")
        print(f"    Time period: {config.hours} hours")
        print(f"    Cadence: {config.cadence_minutes} minutes")
        print(f"    Timepoints: {n_points}")
        print()

    # Load or generate data
    if use_synthetic:
        if verbose:
            print("  ğŸ“Š Generating synthetic timeseries...")
        data_pairs = generate_synthetic_timeseries(
            n_points=n_points,
            shape=(256, 256),
            extra_correlation=0.3,
            seed=config.seed
        )
    else:
        if verbose:
            print("  ğŸ“¡ Loading real AIA data...")
        # TODO: Implement real data loading
        # For now: Fallback to synthetic
        print("  âš ï¸  Real data not yet implemented, using synthetic")
        data_pairs = generate_synthetic_timeseries(
            n_points=n_points,
            shape=(256, 256),
            extra_correlation=0.3,
            seed=config.seed
        )

    # Analyze each timepoint
    timeseries = []

    if verbose:
        print(f"\n  ğŸ”¬ Analyzing {n_points} timepoints...")

    for i, (img_1, img_2, timestamp) in enumerate(data_pairs):
        if verbose and (i + 1) % 5 == 0:
            print(f"     Timepoint {i+1}/{n_points}...")

        tp_result = analyze_timepoint(img_1, img_2, timestamp, config)
        timeseries.append(tp_result)

    # Control tests on first timepoint
    if verbose:
        print("\n  ğŸ§ª Running control tests...")

    first_img_1, first_img_2, _ = data_pairs[0]
    controls = run_all_controls(
        first_img_1, first_img_2,
        seed=config.seed,
        bins=config.bins,
        verbose=False
    )

    # Spatial analysis on first timepoint
    if verbose:
        print("  ğŸ—ºï¸  Creating spatial analysis...")

    spatial = run_spatial_analysis(first_img_1, first_img_2)

    # Aggregate statistics
    mi_ratios = [tp.mi_ratio for tp in timeseries]
    delta_rings = [tp.delta_mi_ring for tp in timeseries]
    delta_sectors = [tp.delta_mi_sector for tp in timeseries]

    duration = time.time() - start_time

    result = RunResult(
        config=config,
        timeseries=timeseries,
        controls=asdict(controls) if controls else None,
        spatial=spatial,
        mean_mi_ratio=float(np.mean(mi_ratios)),
        std_mi_ratio=float(np.std(mi_ratios)),
        mean_delta_mi_ring=float(np.mean(delta_rings)),
        mean_delta_mi_sector=float(np.mean(delta_sectors)),
        run_timestamp=run_timestamp,
        duration_seconds=duration
    )

    # Save results
    if verbose:
        print("\n  ğŸ’¾ Saving results...")

    save_timeseries_csv(timeseries, output_dir / "timeseries.csv")
    save_controls_json(controls, output_dir / "controls_summary.json")
    save_spatial_maps(spatial, output_dir / "spatial_maps.txt")
    save_run_metadata(result, output_dir / "run_metadata.json")

    if verbose:
        print_run_summary(result)

    return result


# ============================================================================
# CLI
# ============================================================================

def main():
    """Main function."""

    parser = argparse.ArgumentParser(
        description="Solar Seed Real-Run",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python -m solar_seed.real_run --hours 6
  python -m solar_seed.real_run --hours 1 --synthetic
  python -m solar_seed.real_run --wavelengths 171 193
        """
    )
    parser.add_argument("--hours", type=float, default=6.0,
                        help="Analysis time period in hours (default: 6)")
    parser.add_argument("--wavelengths", type=int, nargs=2, default=[193, 211],
                        help="Wavelength pair (default: 193 211)")
    parser.add_argument("--cadence", type=int, default=12,
                        help="Cadence in minutes (default: 12)")
    parser.add_argument("--output", type=str, default="results/real_run",
                        help="Output directory")
    parser.add_argument("--synthetic", action="store_true",
                        help="Use synthetic data")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed")

    args = parser.parse_args()

    config = RunConfig(
        wavelength_1=args.wavelengths[0],
        wavelength_2=args.wavelengths[1],
        hours=args.hours,
        cadence_minutes=args.cadence,
        output_dir=args.output,
        seed=args.seed
    )

    run_real_analysis(config, use_synthetic=args.synthetic)


if __name__ == "__main__":
    main()
